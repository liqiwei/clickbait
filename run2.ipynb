{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python2.7/site-packages (1.15.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python2.7/site-packages (0.23.4)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/site-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/site-packages (from pandas) (1.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python2.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/site-packages (from sklearn) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/site-packages (from scikit-learn->sklearn) (1.15.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python2.7/site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbait_percentage = pd.read_csv('percent_file.csv')\n",
    "dic_source_clickbait = {row['source']: row['percent_clickbait'] for i, row in clickbait_percentage.iterrows()}\n",
    "average_clickbait = clickbait_percentage.percent_clickbait.sum() / len(clickbait_percentage)\n",
    "data = pd.read_csv('edata_all_no_www.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_clickbait(source):\n",
    "    if source.startswith('www.'):\n",
    "        source = source[4:]\n",
    "    if source in dic_source_clickbait:\n",
    "        return dic_source_clickbait[source]\n",
    "    return average_clickbait\n",
    "    \n",
    "data['clickbait_percentage'] = data['source'].apply(add_clickbait)\n",
    "\n",
    "\n",
    "def get_features(data, source_len = 724):\n",
    "    \"\"\"\n",
    "    features for claims\n",
    "    \"\"\"\n",
    "    dic_f = {} # claimCount -> features\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        row = data.iloc[i]\n",
    "        stance = row['articleHeadlineStance']\n",
    "        stance_id = -1 if stance == 'against' else 0 if stance == 'observing'\\\n",
    "            else 1\n",
    "        source = row.sourceCount - 1 # 1-index to 0-index\n",
    "        claim = row.claimCount\n",
    "        \n",
    "        if claim not in dic_f: dic_f[claim] = np.zeros((source_len,))\n",
    "        dic_f[claim][source] = stance_id\n",
    "    \n",
    "    #claims = dic_f.keys()\n",
    "    return dic_f\n",
    "\n",
    "\n",
    "def extract_truth_labels(data):\n",
    "    claims = sorted(data.claimCount.unique().tolist())\n",
    "    l = [''] * len(claims)\n",
    "    for i in range(len(data)):\n",
    "        row = data.iloc[i]\n",
    "        truth = row.claimTruth\n",
    "        claim = row.claimCount\n",
    "        claimIdx = claims.index(claim)\n",
    "        l[claimIdx] = truth        \n",
    "    return (claims, l)\n",
    "\n",
    "\n",
    "def build_veracity_prediction_matrix():\n",
    "    dic_f = get_features(data)\n",
    "        \n",
    "    (claims, veracity) = extract_truth_labels(data)\n",
    "    \n",
    "    n = len(claims)\n",
    "    m = dic_f.items()[0][1].shape[0]\n",
    "    \n",
    "    F = np.zeros((n, m))\n",
    "    for i, c in enumerate(claims): F[i, :] = dic_f[c]\n",
    "    \n",
    "    return (claims, F, veracity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "claims, F, vera = build_veracity_prediction_matrix()\n",
    "clf = sklearn.linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5864252410305042\n",
      "0.009686965948098532\n"
     ]
    }
   ],
   "source": [
    "cross_var = 8\n",
    "\n",
    "print(np.mean(sklearn.model_selection.cross_val_score(clf, F, vera, cv=cross_var)))\n",
    "print(np.var(sklearn.model_selection.cross_val_score(clf, F, vera, cv=cross_var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = F.copy()\n",
    "for i, row in data.iterrows():\n",
    "    source_index = row['sourceCount'] - 1\n",
    "    percent_clickbait = row['clickbait_percentage']\n",
    "    G[:, source_index] = F[:, source_index] * (1 - (percent_clickbait * 0.01))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_g = sklearn.linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6131954322743797\n",
      "0.013387398431191792\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sklearn.model_selection.cross_val_score(clf_g, G, vera, cv=cross_var)))\n",
    "print(np.var(sklearn.model_selection.cross_val_score(clf_g, G, vera, cv=cross_var)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different cross validation value and do the paired T test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5754096638655462 0.11670510696250525\n",
      "0.6076663165266106 0.13459689962025628\n",
      "Ttest_1sampResult(statistic=1.7345952654975938, pvalue=0.09900676051044668)\n"
     ]
    }
   ],
   "source": [
    "#cross_var = 5\n",
    "#cross_var = 10\n",
    "#cross_var = 15\n",
    "cross_var = 20\n",
    "leave_one_out = sklearn.model_selection.LeaveOneOut()\n",
    "original_accuracies = sklearn.model_selection.cross_val_score(clf, F, vera, cv=cross_var)\n",
    "new_accuracies = sklearn.model_selection.cross_val_score(clf_g, G, vera, cv=cross_var)\n",
    "print np.mean(original_accuracies), np.std(original_accuracies)\n",
    "print np.mean(new_accuracies), np.std(new_accuracies)\n",
    "import scipy\n",
    "print scipy.stats.ttest_1samp(new_accuracies - original_accuracies, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
